Index: BoostedDT.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># RR\r\nimport webbrowser\r\n\r\nfrom sklearn.tree import export_graphviz\r\n\r\nimport Preprocessing\r\nfrom sklearn.model_selection import GridSearchCV\r\nfrom sklearn.model_selection import StratifiedShuffleSplit\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn.metrics import confusion_matrix\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\nfrom sklearn.ensemble import GradientBoostingClassifier\r\nfrom sklearn.metrics import roc_auc_score\r\nimport xgboost as xgb\r\n\r\n#%%-----------------------------------------------------------------------\r\nimport os\r\nos.environ[\"PATH\"] += os.pathsep + 'C:\\Program Files (x86)\\Graphviz\\bin'\r\n#%%-----------------------------------------------------------------------\r\nfrom pydotplus import graph_from_dot_data\r\nmodel = Preprocessing.crash_model\r\n\r\n\r\nclass xgboost:  # class\r\n    def __init__(self, data):  # to call self\r\n        # data is the entire data matrix\r\n        self.xtrain = data.iloc[:,:-1]\r\n        self.ytrain = data.iloc[:,-1]\r\n\r\n\r\n    def accuracy(self):  # this makes the model and finds the accuracy, confusion matrix, and prints the decision tree\r\n        # 13 lines of code - 4 copied, 1 modified, 9 myself\r\n        clf = xgb.XGBClassifier(n_estimators=500, # these are the parameters - were adjusted\r\n                                learning_rate=0.01, # tried 0.01,0.05,0.1,0.2\r\n                                max_depth=50, # tried 10, 25, 50\r\n                                min_samples_split=2,\r\n                                min_samples_leaf=1,\r\n                                #warm_start=True,\r\n                                reg_lambda = 10, # tried 1, 10, 100\r\n                                reg_alpha = 10 # tried 1, 10, 100\r\n                                       )\r\n        X_train, X_test, y_train, y_test = train_test_split(self.xtrain, self.ytrain, test_size=0.3, random_state=10) # split data up\r\n        clf.fit(X_train, y_train) # fit model to training data\r\n        y_pred = clf.predict(X_test)  # predict testing data\r\n        self.roc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]) # get AUC value\r\n        self.acc = accuracy_score(y_test, y_pred) * 100  # get the accuracy of the model\r\n        print('The AUC of the model is:', self.roc)\r\n        print('The classification accuracy is:', self.acc)\r\n\r\n        # Dr. Jafari code - 6 copied, not modified\r\n        # Selecting important features. Lines 33-68 are from Dr. Jafari's code and were updated accordingly\r\n        importances = clf.feature_importances_\r\n        # convert the importances into one-dimensional 1darray with corresponding df column names as axis labels\r\n        f_importances = pd.Series(importances, self.xtrain.columns)\r\n        # sort the array in descending order of the importances\r\n        f_importances.sort_values(ascending=False, inplace=True)\r\n        f_importances.plot(x='Features', y='Importance', kind='bar', figsize=(16, 9), rot=90, fontsize=15, color='r')\r\n        plt.tight_layout()\r\n        #plt.title('Feature Importance', fontsize=20)\r\n        plt.show()\r\n\r\n\r\n        # Dr. Jafari code - 3 copied, not modified\r\n        conf_matrix = confusion_matrix(y_test, y_pred) # make confusion matrix\r\n        class_names = self.ytrain.unique()  # get the class names\r\n        df_cm = pd.DataFrame(conf_matrix, index=class_names, columns=class_names)\r\n\r\n        # sensitivity and specificity - 4 copied and modified RR\r\n        sensitivity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])  # calculate sensitivity\r\n        print('Sensitivity : ', sensitivity)\r\n        specificity = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])  # calculate specificity\r\n        print('Specificity : ', specificity)\r\n\r\n        # Dr. Jafari Code - 9 copied, not modified, 1 line myself\r\n        plt.figure(figsize=(5, 5))\r\n        hm = sns.heatmap(df_cm, cbar=False, annot=True, square=True, fmt='d', annot_kws={'size': 20},\r\n                         yticklabels=df_cm.columns, xticklabels=df_cm.columns)\r\n        hm.yaxis.set_ticklabels(hm.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=20)\r\n        hm.xaxis.set_ticklabels(hm.xaxis.get_ticklabels(), rotation=0, ha='right', fontsize=20)\r\n        plt.ylabel('True label', fontsize=20)\r\n        plt.xlabel('Predicted label', fontsize=20)\r\n        plt.title('Extreme Gradient Boosted DT Confusion Matrix')\r\n        plt.tight_layout()\r\n        plt.show()\r\n\r\n        # Dr. Jafari - 5 lines copied not modified\r\n        # dot_data = export_graphviz(clf, filled=True, rounded=True, class_names=class_names,\r\n        #                            feature_names=self.xtrain.iloc[:, :].columns, out_file=None)\r\n        #\r\n        # graph = graph_from_dot_data(dot_data)\r\n        # graph.write_pdf(\"GBoost Decision Tree\")\r\n        # webbrowser.open_new(r'GBoost Decision Tree')\r\n\r\n        # 1 line myself\r\n        return self.roc  # return the accuracy\r\n\r\n# 2 lines myself\r\nm = xgboost(model) # put model into class\r\nm.accuracy() # run
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/BoostedDT.py b/BoostedDT.py
--- a/BoostedDT.py	(revision 34687b293815aff3243bfbe62702d96862feefb2)
+++ b/BoostedDT.py	(date 1618969975275)
@@ -16,11 +16,12 @@
 from sklearn.metrics import roc_auc_score
 import xgboost as xgb
 
+
 #%%-----------------------------------------------------------------------
 import os
-os.environ["PATH"] += os.pathsep + 'C:\Program Files (x86)\Graphviz\bin'
+os.environ["PATH"] += os.pathsep + "C:\Program Files (x86)\Graphviz\bin"
 #%%-----------------------------------------------------------------------
-from pydotplus import graph_from_dot_data
+#from graphviz.pydotplus import graph_from_dot_data
 model = Preprocessing.crash_model
 
 
Index: randomforest.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Arianna - random forest\r\n\r\n# Importing necessary libraries\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn.metrics import roc_auc_score\r\nfrom sklearn import tree\r\nimport pydotplus\r\nimport collections\r\nimport os\r\nos.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\r\n\r\n# Getting preprocessed data\r\nimport Preprocessing\r\nmodel = Preprocessing.crash_model\r\n\r\n#  Defining random forest algorithm as a class. Lines 20-30 are from Ryeanne's code & were updated accordingly\r\nclass randforest:  # class\r\n    def __init__(self, data):  # to call self\r\n        # data is the entire data matrix\r\n        self.xtrain = data.iloc[:,:-1]\r\n        self.ytrain = data.iloc[:,-1]\r\n\r\n        clf = RandomForestClassifier(n_estimators=100)\r\n\r\n        X_train, X_test, y_train, y_test = train_test_split(self.xtrain, self.ytrain, test_size=0.3,\r\n                                                            random_state=100)  # split data up\r\n        clf.fit(X_train, y_train)  # fit model to training data\r\n\r\n        # Selecting important features. Lines 33-68 are from Dr. Jafari's code and were updated accordingly\r\n        importances = clf.feature_importances_\r\n\r\n        # convert the importances into one-dimensional 1darray with corresponding df column names as axis labels\r\n        f_importances = pd.Series(importances, self.xtrain.columns)\r\n\r\n        # sort the array in descending order of the importances\r\n        f_importances.sort_values(ascending=False, inplace=True)\r\n\r\n        # make the bar Plot from f_importances\r\n        f_importances.plot(x='Features', y='Importance', kind='bar', figsize=(20, 10), rot=90, fontsize=15)\r\n\r\n        # show the plot\r\n        plt.title(\"Important Features\", fontsize=30)\r\n        plt.xlabel(\"Feature Names\", fontsize=20)\r\n        plt.gcf().subplots_adjust(bottom=0.35)\r\n        plt.show()\r\n\r\n        # Generating model with important features\r\n        self.Xtrain = data.iloc[:, :-4]\r\n        newX_train, newX_test, y_train, y_test = train_test_split(self.Xtrain, self.ytrain, test_size=0.3,\r\n                                                            random_state=100)\r\n\r\n        # %%-----------------------------------------------------------------------\r\n        # perform training with random forest with k columns\r\n        # specify random forest classifier\r\n        clf_k_features = RandomForestClassifier(n_estimators=100)\r\n\r\n        # train the model\r\n        clf_k_features.fit(newX_train, y_train)\r\n\r\n        # %%----------------------------------------------------------------------\r\n        # predicton on test using all features\r\n        y_pred = clf.predict(X_test)\r\n        y_pred_score = clf.predict_proba(X_test)\r\n\r\n        # prediction on test using k features\r\n        y_pred_k_features = clf_k_features.predict(newX_test)\r\n        y_pred_k_features_score = clf_k_features.predict_proba(newX_test)\r\n\r\n        # Testing accuracy. Lines 71-79 were from Reyanne's code and were updated accordingly\r\n        self.roc = roc_auc_score(y_test, y_pred_score[:, 1] * 100) # get AUC value\r\n        self.acc = accuracy_score(y_test, y_pred) * 100  # get the accuracy of the model\r\n        print(\"Results using all features: \")\r\n        print('The AUC of the model is:', self.roc)\r\n        print('The classification accuracy is:', self.acc)\r\n\r\n        self.roc = roc_auc_score(y_test, y_pred_k_features_score[:, 1]) * 100  # get AUC value\r\n        self.acc = accuracy_score(y_test, y_pred_k_features) * 100  # get the accuracy of the model\r\n        print(\"Results using important features: \")\r\n        print('The AUC of the important features model is:', self.roc)\r\n        print('The classification accuracy for the important features is:', self.acc)\r\n\r\n        # %%-----------------------------------------------------------------------\r\n        # confusion matrix for gini model. Lines 83-125 are from Dr. Jafari's code & were updated accordingly\r\n        conf_matrix = confusion_matrix(y_test, y_pred)\r\n        class_names = self.ytrain.unique()\r\n        df_cm = pd.DataFrame(conf_matrix, index=class_names, columns=class_names)\r\n\r\n        # sensitivity and specificity - 4 copied and modified RR\r\n        sensitivity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])  # calculate sensitivity\r\n        print('Sensitivity : ', sensitivity)\r\n        specificity = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])  # calculate specificity\r\n        print('Specificity : ', specificity)\r\n\r\n\r\n        plt.figure(figsize=(5, 5))\r\n        hm = sns.heatmap(df_cm, cbar=False, annot=True, square=True, fmt='d', annot_kws={'size': 20},\r\n                         yticklabels=df_cm.columns, xticklabels=df_cm.columns)\r\n\r\n        hm.yaxis.set_ticklabels(hm.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=20)\r\n        hm.xaxis.set_ticklabels(hm.xaxis.get_ticklabels(), rotation=0, ha='right', fontsize=20)\r\n        plt.ylabel('True label', fontsize=20)\r\n        plt.xlabel('Predicted label', fontsize=20)\r\n        plt.title('Random Forest Confusion Matrix Gini Model')\r\n        # Show heat map\r\n        plt.tight_layout()\r\n\r\n        # %%-----------------------------------------------------------------------\r\n\r\n        # confusion matrix for entropy model\r\n        conf_matrix = confusion_matrix(y_test, y_pred_k_features)\r\n        class_names = self.ytrain.unique()\r\n        df_cm = pd.DataFrame(conf_matrix, index=class_names, columns=class_names)\r\n\r\n        plt.figure(figsize=(5, 5))\r\n\r\n        hm = sns.heatmap(df_cm, cbar=False, annot=True, square=True, fmt='d', annot_kws={'size': 20},\r\n                         yticklabels=df_cm.columns, xticklabels=df_cm.columns)\r\n\r\n        hm.yaxis.set_ticklabels(hm.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=20)\r\n        hm.xaxis.set_ticklabels(hm.xaxis.get_ticklabels(), rotation=0, ha='right', fontsize=20)\r\n        plt.ylabel('True label', fontsize=20)\r\n        plt.xlabel('Predicted label', fontsize=20)\r\n        plt.title('Random Forest Confusion Matrix Entropy Model')\r\n        # Show heat map\r\n        plt.tight_layout()\r\n        plt.show()\r\n\r\n        # Printing first tree from all features. 4/4 written by Arianna\r\n        plt.figure(figsize=(15, 10))\r\n        plt.title(\"All Features Random Forest Tree No.1\")\r\n        tree.plot_tree(clf.estimators_[0], filled=True, max_depth=3)\r\n        plt.show()\r\n\r\n        # Printing first tree from important features. 4/4 written by Arianna\r\n        plt.figure(figsize=(15,10))\r\n        plt.title(\"Important Features Random Forest Tree No.1\")\r\n        tree.plot_tree(clf_k_features.estimators_[0], filled=True, max_depth=3)\r\n        plt.show()\r\n\r\n\r\n# Running the model\r\nm = randforest(model)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/randomforest.py b/randomforest.py
--- a/randomforest.py	(revision 34687b293815aff3243bfbe62702d96862feefb2)
+++ b/randomforest.py	(date 1618970841506)
@@ -10,7 +10,7 @@
 from sklearn.metrics import accuracy_score
 from sklearn.metrics import roc_auc_score
 from sklearn import tree
-import pydotplus
+#import pydotplus
 import collections
 import os
 os.environ["PATH"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'
